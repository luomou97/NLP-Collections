\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\citation{DistilBert}
\citation{Transformer}
\citation{Adafactor}
\citation{Bert}
\citation{Albert}
\citation{T5}
\citation{Reformer}
\citation{Glu-variants}
\citation{SimCLR}
\citation{Sentence-T5}
\citation{Poly-Encoders}
\citation{Roberta}
\citation{Vilbert}
\citation{MoCo}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}NLP团队总览}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}HuggingFace抱抱脸}{3}{section.1.1}\protected@file@percent }
\newlabel{com:HuggingFace}{{1.1}{3}{HuggingFace抱抱脸}{section.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}DeepMind}{3}{section.1.2}\protected@file@percent }
\newlabel{com:DeepMind}{{1.2}{3}{DeepMind}{section.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Google AI谷歌人工智能}{3}{section.1.3}\protected@file@percent }
\newlabel{com:GoogleAI}{{1.3}{3}{Google AI谷歌人工智能}{section.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}FAIR facebook人工智能研究院}{3}{section.1.4}\protected@file@percent }
\newlabel{com:FAIR}{{1.4}{3}{FAIR facebook人工智能研究院}{section.1.4}{}}
\citation{MoCo-2}
\citation{GPT-1}
\citation{GPT-2}
\citation{Spare-Transformer}
\citation{GPT-3}
\citation{CLIP}
\citation{LightGBM}
\citation{MT-DNN}
\citation{UniLM}
\citation{WeightNormalization}
\citation{UnifiedQA}
\citation{Unicorn}
\citation{CPM-1}
\citation{CPM-2}
\citation{M6}
\citation{SimCSE}
\citation{CPM-2}
\citation{M6}
\citation{ERNIE-1.0}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}OpenAI}{4}{section.1.5}\protected@file@percent }
\newlabel{com:OpenAI}{{1.5}{4}{OpenAI}{section.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Microsoft Research微软研究院}{4}{section.1.6}\protected@file@percent }
\newlabel{com:MSR}{{1.6}{4}{Microsoft Research微软研究院}{section.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}AllenAI}{4}{section.1.7}\protected@file@percent }
\newlabel{com:AllenAI}{{1.7}{4}{AllenAI}{section.1.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Tsinghua}{4}{section.1.8}\protected@file@percent }
\newlabel{com:Tsinghua}{{1.8}{4}{Tsinghua}{section.1.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.9}BAAI 北京智源人工智能研究院}{4}{section.1.9}\protected@file@percent }
\newlabel{com:BAAI}{{1.9}{4}{BAAI 北京智源人工智能研究院}{section.1.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.10}Alibaba阿里巴巴}{4}{section.1.10}\protected@file@percent }
\newlabel{com:Alibaba}{{1.10}{4}{Alibaba阿里巴巴}{section.1.10}{}}
\citation{ERNIE-2.0}
\citation{PLATO-1}
\citation{PLATO-2}
\citation{ERNIE-vil}
\citation{Knover}
\citation{Bert-wwm}
\citation{Electra}
\citation{MacBert}
\@writefile{toc}{\contentsline {section}{\numberline {1.11}Baidu AIG 百度人工智能研究院}{5}{section.1.11}\protected@file@percent }
\newlabel{com:Baidu AIG}{{1.11}{5}{Baidu AIG 百度人工智能研究院}{section.1.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.12}IFlyTek 科大讯飞}{5}{section.1.12}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}基于Encoder的模型}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}基于Decoder的模型}{9}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}基于Encoder-Decoder的模型}{11}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Transformer}{11}{section.4.1}\protected@file@percent }
\newlabel{model:Transformer}{{4.1}{11}{Transformer}{section.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Transformer模型架构}}{11}{figure.4.1}\protected@file@percent }
\newlabel{Trm:architecture}{{4.1}{11}{Transformer模型架构}{figure.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}T5}{12}{section.4.2}\protected@file@percent }
\newlabel{model:T5}{{4.2}{12}{T5}{section.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Sentence T5 (ST5)}{12}{section.4.3}\protected@file@percent }
\newlabel{model:ST5}{{4.3}{12}{Sentence T5 (ST5)}{section.4.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}对比学习}{13}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}多模态}{15}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{Attention}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}基本组件}{17}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Attention注意力机制}{17}{section.7.1}\protected@file@percent }
\newlabel{component:Attention}{{7.1}{17}{Attention注意力机制}{section.7.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Normalization归一化}{17}{section.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}WeightNormalization权重归一化}{17}{subsection.7.2.1}\protected@file@percent }
\newlabel{component:WeightNormalization}{{7.2.1}{17}{WeightNormalization权重归一化}{subsection.7.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}BatchNormalization (BN) 批量归一化}{17}{subsection.7.2.2}\protected@file@percent }
\newlabel{component:BN}{{7.2.2}{17}{BatchNormalization (BN) 批量归一化}{subsection.7.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}LayerNormalization (LN) 层归一化}{17}{subsection.7.2.3}\protected@file@percent }
\newlabel{component:LN}{{7.2.3}{17}{LayerNormalization (LN) 层归一化}{subsection.7.2.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}提升技巧}{19}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Optimizer优化器}{21}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}集成学习}{23}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {.1}Acronyms缩略词}{25}{section.Alph0.1}\protected@file@percent }
\bibdata{refs}
\@writefile{toc}{\contentsline {section}{\numberline {.2}Glossary词汇表}{26}{section.Alph0.2}\protected@file@percent }
\bibcite{DistilBert}{1}
\bibcite{Transformer}{2}
\bibcite{Adafactor}{3}
\bibcite{Bert}{4}
\bibcite{Albert}{5}
\bibcite{T5}{6}
\bibcite{Reformer}{7}
\bibcite{Glu-variants}{8}
\bibcite{SimCLR}{9}
\bibcite{Sentence-T5}{10}
\bibcite{Poly-Encoders}{11}
\bibcite{Roberta}{12}
\bibcite{Vilbert}{13}
\bibcite{MoCo}{14}
\bibcite{MoCo-2}{15}
\bibcite{GPT-1}{16}
\bibcite{GPT-2}{17}
\bibcite{Spare-Transformer}{18}
\bibcite{GPT-3}{19}
\bibcite{CLIP}{20}
\bibcite{LightGBM}{21}
\bibcite{MT-DNN}{22}
\bibcite{UniLM}{23}
\bibcite{WeightNormalization}{24}
\bibcite{UnifiedQA}{25}
\bibcite{Unicorn}{26}
\bibcite{CPM-1}{27}
\bibcite{CPM-2}{28}
\bibcite{M6}{29}
\bibcite{SimCSE}{30}
\bibcite{ERNIE-1.0}{31}
\bibcite{ERNIE-2.0}{32}
\bibcite{PLATO-1}{33}
\bibcite{PLATO-2}{34}
\bibcite{ERNIE-vil}{35}
\bibcite{Knover}{36}
\bibcite{Bert-wwm}{37}
\bibcite{Electra}{38}
\bibcite{MacBert}{39}
\bibcite{Attention}{40}
\bibstyle{ieeetr}
